# N-word-frequency

C++ programs used to generate N-word-frequency database from the <a href="https://books.google.com/ngrams">Google Books Ngram Corpus</a>.


# Requirements

## Download N-gram Google Books

First, download <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">The Google Books Ngram Corpus</a> you need (with the corresponding totalcounts file) and put all the files in a directory. If you use MacOS, you can download them with -- --.

## On Windows 10

The programs work on a Linux environment. You can install a <a href="https://www.virtualbox.org/wiki/Downloads">virtual machine</a> to use Ubuntu for example, or you can install Ubuntu Bash.

Installing Ubuntu Bash:
1. You need to have a Build version greater than 14393 and Windows 10 64-bits. !!Image ici!!
2. Open Settings app and go to Update & Security -> For Developers and choose the “Developer Mode” radio button.
3. Go to the Control Panel -> Programs and click “Turn Windows feature on or off”. Enable “Windows Subsystem for Linux(Beta)”. When you click OK, you will be prompted to reboot. Click “Restart Now” to reboot your PC.
4. Open the Microsoft Store, search "Ubuntu 18.04 LTS" and download it. 
5. Open the Ubuntu 18.04 terminal, enter an user name and a password.

You can now read the "On Linux" part to complete others installations.

## On Linux

-S'il y a des problèmes de fichier les mettre en format unix:  sudo apt-get install dos2unix puis dos2unix nom_fichier -

1. Open a terminal.
2. Run `sudo apt-get update`
3. Install g++ `sudo apt-get install g++`
4. Install make `sudo apt-get install make`
5. <a href="https://zlib.net/zlib1211.zip">Download zlib</a> and extract it to the same programs directory.
6. Go to the zlib directory and run ./configure: `cd zlib1211/zlib-1.2.11/` and `./configure`
7. Run `sudo make install`
8. `cd ../..` and `sudo chmod 777 make_all.sh`
9. `./make_all.sh`

## On MacOS

--todo - surement pareil que sur linux--


# How to use the different programs
For each programs, there are a multithreading and a non-multithreading version. Multithreading is used to reduce computation time. We recommand to use the multithreading version if there are a lot of files to process.

## a_generate_files
From .gz files, it generates ngrams files containing for each unique ngram its  :
* number of years
* total occurrence through the years
* number of volume through the years
* mean year ponderated by the total of occurrences
* mean year ponderated by the number of volumes
* year max
* year min
* occurence max
* occurence min
* number of volume max
* number of volume min

\tTake into account only ngrams with words tagged with :\n\
\tNOUN, VERB, ADJ, ADV, PRON, DET, ADP, CONJ, PRT and words different \
than:\n\t',', '.', '?', '!', '...', ';', ':', '\"', ' ', '''\n\n"

Put in the config file :
a_generate_file:
path_to_gz_files = /path_to_the_googlebooks_ngram_corpus_gz_files/
path_to_output_files = /path_to_where_you_want_the_output_files/
nb_ngram = number of ngram in the .gz files
min_year = the minimum year you want
END

Example :
<pre><code>
a_generate_file:
path_to_gz_files = /mnt/c/Users/Me/Documents/N-word-frequency/files_gz/
path_to_output_files = /mnt/c/Users/Me/Documents/N-word-frequency/output_files_a_generate_files/
nb_ngram = 4
min_year = 1970
END
</code></pre>

## b_calcul_total_occurrences

Uses the files generated by a_generate_files to compute the total number of ngrams (sum of all unique ngrams' occurrence). Read the total number of volumes with the totalcounts file. Writes these two numbers in a file.

Put in the config file:
b_calcul_total_occurrences:
output_file_name = 
totalcount_file = 
path_to_treated_files = 
min_year = 
END

Example :
<pre><code>
b_calcul_total_occurrences:
output_file_name = /mnt/c/Users/Me/Documents/N-word-frequency/total_occurrences_fr_4grams.txt
totalcount_file = /mnt/c/Users/Me/Documents/N-word-frequency/googlebooks-fre-all-totalcounts-20120701.txt
path_to_treated_files = /mnt/c/Users/Me/Documents/N-word-frequency/output_files_a_generate_files/
min_year = 1970
END
</code></pre>

## c_calcul_frequences

Compute the frequence of each unique ngram generated by a_generate_files, thanks to the file generated by b_calcul_total_occurrences.

Example :
<pre><code>
c_calcul_frequences:
total_occurrences_files = /mnt/c/Users/Me/Documents/N-word-frequency/total_occurrences_fr_4grams.txt
path_to_treated_files = /mnt/c/Users/Me/Documents/N-word-frequency/output_files_a_generate_files/
path_to_output_files = /mnt/c/Users/Me/Documents/N-word-frequency/output_files_c_calcul_frequences/
END
</code></pre>

## d_calcul_frequences_tag_grams

Compute the frequence of each tag-grams from the Google Books Ngram Corpus .gz files. They are named \_ADJ\_, \_ADP\_, \_ADV\_, \_CONJ\_, \_DET\_, \_NOUN\_, \_PRON\_, \_PRT\_, \_VERB\_. Put them all in the same directory. The output file is a .csv containing, for each tag-gram :
* number of years
* total occurrence through the years
* number of volume through the years
* mean year ponderated by the total of occurrences
* mean year ponderated by the number of volumes
* year max
* year min
* occurence max
* occurence min
* number of volume max
* number of volume min
* total occurrence's frequence
* number of volume's frequence

Example :
<pre><code>
d_calcul_frequences_tag_grams:
output_file_name = /mnt/c/Users/Me/Documents/N-word-frequency/fr_4grams_tag_grams_frequencies.csv
totalcount_file = /mnt/c/Users/Me/Documents/N-word-frequency/googlebooks-fre-all-totalcounts-20120701.txt
path_to_gz_pos_files = /mnt/c/Users/Me/Documents/N-word-frequency/files_gz/pos/
nb_ngram = 4
min_year = 1970
END
</code></pre>